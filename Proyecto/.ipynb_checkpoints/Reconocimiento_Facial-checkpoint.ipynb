{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d8ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 262)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(828, 552)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(184, 273)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(589, 844)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(626, 798)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(222, 332)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(271, 186)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(195, 259)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(299, 168)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(626, 710)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(345, 505)\n",
      "torch.Size([1, 3, 160, 160])\n",
      "(735, 811)\n",
      "torch.Size([1, 3, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "#from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "#face detector\n",
    "detector = MTCNN(keep_all=True)\n",
    "\n",
    "# This return a pretrained model that is vggface2\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# Read data from folder\n",
    "dataset = datasets.ImageFolder('Conocidos') # photos folder path \n",
    "\n",
    "#dataset\n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names correspoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    print(img.size)\n",
    "    #Unlike other implementations, calling a facenet-pytorch MTCNN object directly with an image (i.e., using the forward method for those familiar with pytorch) will return torch tensors containing the detected face(s), rather than just the bounding boxes. This is to enable using the module easily as the first stage of a facial recognition pipeline, in which the faces are passed directly to an additional network or algorithm.\n",
    "    face, prob = detector(img, return_prob=True)\n",
    "    #print(\"face: \",face,\" prob:\",prob)\n",
    "    if face is not None and prob>0.92:\n",
    "        print(face.squeeze(1).shape)\n",
    "        #emb = resnet(face.unsqueeze(0))\n",
    "        emb = resnet(face.squeeze(1)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225acf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esc pressed, closing...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "detector = MTCNN(keep_all=True)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "# loading data.pt file\n",
    "load_data = torch.load('data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] \n",
    "\n",
    "cam = cv2.VideoCapture(0) \n",
    "\n",
    "while cam.isOpened():\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"fail to grab frame, try again\")\n",
    "        # The next frame is not ready, so we try to read it again\n",
    "        cap.set(cv2.CV_CAP_PROP_POS_FRAMES, pos_frame-1)\n",
    "        print(\"frame is not ready\")\n",
    "        # It is better to wait for a while for the next frame to be ready\n",
    "        cv2.waitKey(1000)\n",
    "        break\n",
    "        \n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = detector(img, return_prob=True) \n",
    "    \n",
    "    if img_cropped_list is not None:\n",
    "        #return boxed faces\n",
    "        boxes, _ = detector.detect(img)\n",
    "                \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i]\n",
    "                #print(type(box), box)\n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                \n",
    "                if min_dist<0.90:\n",
    "                    #bgr\n",
    "                    #frame = cv2.putText(frame, str(name)+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (63, 0, 252),1, cv2.LINE_AA)\n",
    "                    frame = cv2.putText(frame, str(name), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (63, 0, 252),1, cv2.LINE_AA)\n",
    "\n",
    "                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (13,214,53), 2)\n",
    "\n",
    "    cv2.imshow(\"IMG\", frame)\n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('data2/'+name):\n",
    "            os.mkdir('data2/'+name)\n",
    "            \n",
    "        img_name = \"data2/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa968910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
